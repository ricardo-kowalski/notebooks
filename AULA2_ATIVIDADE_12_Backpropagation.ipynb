{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AULA2_ATIVIDADE_12_Backpropagation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardo-kowalski/notebooks/blob/master/AULA2_ATIVIDADE_12_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DROmonaqF9v",
        "colab_type": "text"
      },
      "source": [
        "#Perceptron Multicamadas e Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02naKK51qbml",
        "colab_type": "text"
      },
      "source": [
        "O programa a seguir tem por objetivo construir uma rede neura do tipo Multilayer Perceptron além de treinar essa rede utilizando o algoritmo de retropropagação (Backpropagation), divide-se em 4 partes principais, são elas:\n",
        "\n",
        "\n",
        "*   Rede: \n",
        " * ligações entre os neurônios, função de ativação utilizada e operações de retropropagação baseadas nas derivadas parciais pré-definidas; \n",
        "*   Camadas:\n",
        " * define um comportamento específico para cada conjunto de neurônios pertencentes à uma mesma camada;\n",
        "*  Neurônio:\n",
        " * define as funções realizadas em cada neurônio de acordo com sua camada tato na propagação quanto na retropropagação;\n",
        "* I/O\n",
        " * definição das entradas , saídas, camadas, neurônios, pesos, bem como a quantidade de cada uma;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msG_T55mqNuN",
        "colab_type": "text"
      },
      "source": [
        "Importando as bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-8WLvLLm_8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random   #usada caso alguns valores não sejam declarados na chamada das classes\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndaZ8KvlogI2",
        "colab_type": "text"
      },
      "source": [
        ">![picture](https://matthewmazur.files.wordpress.com/2018/03/neural_network-7.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnLl7ttlwkxU",
        "colab_type": "text"
      },
      "source": [
        "A primeira parte do código define a classe 'NeuralNetwork' que recebe os argumentos:\n",
        "\n",
        "\n",
        "**num_inputs**\n",
        "> Número\n",
        ">$n$\n",
        "de entradas \n",
        ">$x_1, x_2, ... , x_n$\n",
        "da rede\n",
        "> (no exemplo temos apenas 2 entradas definidas por \n",
        ">$i_1$ \n",
        "e \n",
        ">$i_2$\n",
        ")\n",
        "\n",
        "**num_hidden**\n",
        "> Número de neurônios da camada oculta (hidden)\n",
        ">$h_1, h_2, ... , h_n$\n",
        "da rede\n",
        "> (no exemplo são dois:\n",
        ">$h_1$ \n",
        "e \n",
        ">$h_2$\n",
        ")\n",
        "\n",
        "\n",
        "**num_outputs**\n",
        "> Número de neurônios da camada de saída (output)\n",
        ">$o_1, o_2, ... , o_n$\n",
        "da rede\n",
        "> (no exemplo são dois:\n",
        ">$o_1$ \n",
        "e \n",
        ">$o_2$\n",
        ") \n",
        "\n",
        "**hidden_layer_weights**\n",
        ">Pesos da primeira camada\n",
        ">$w_1, w_2,w_3 , w_4$ \n",
        "\n",
        "**hidden_layer_bias**\n",
        "> Valor do viés \n",
        ">$b_1$\n",
        ">da primeira camada (bias)\n",
        "\n",
        "**output_layer_weights**\n",
        ">Pesos da segunda camada\n",
        ">$w_5, w_6,w_7 , w_8$ \n",
        "\n",
        "**output_layer_bias**\n",
        "> Valor do viés \n",
        ">$b_2$\n",
        ">da segunda camada (bias)\n",
        "\n",
        "Obs.: A  rede da imagem acima possui apenas duas camadas, uma camada oculta(hidden layer) e uma camada de saída (output layer), note que as entradas em verde não fazem parte de uma camada.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awe2MVkanbUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    LEARNING_RATE = 0.5\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n",
        "        self.num_inputs = num_inputs\n",
        "\n",
        "        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
        "        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
        "\n",
        "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
        "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
        "    \n",
        "    #Caso os pesos entre a entrada e a primeira camada não estejam definidos em 'hidden_layer_weights', a função escolhe seus valores iniciais aleatóriamente\n",
        "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
        "        weight_num = 0\n",
        "        for h in range(len(self.hidden_layer.neurons)):\n",
        "            for i in range(self.num_inputs):\n",
        "                if not hidden_layer_weights:\n",
        "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
        "                else:\n",
        "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
        "                weight_num += 1\n",
        "    \n",
        "    #Caso os pesos entre a camada oculta e a camada de ssaída não estejam definidos em 'output_layer_weights', a função escolhe seus valores iniciais aleatóriamente            \n",
        "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
        "        weight_num = 0\n",
        "        for o in range(len(self.output_layer.neurons)):\n",
        "            for h in range(len(self.hidden_layer.neurons)):\n",
        "                if not output_layer_weights:\n",
        "                    self.output_layer.neurons[o].weights.append(random.random())\n",
        "                else:\n",
        "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
        "                weight_num += 1\n",
        "\n",
        "    def inspect(self):\n",
        "        print('------')\n",
        "        print('* Inputs: {}'.format(self.num_inputs))\n",
        "        print('------')\n",
        "        print('Hidden Layer')\n",
        "        self.hidden_layer.inspect()\n",
        "        print('------')\n",
        "        print('* Output Layer')\n",
        "        self.output_layer.inspect()\n",
        "        print('------')\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
        "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
        "\n",
        "    # Usa aprendizado on-line, ou seja, atualiza os pesos após cada caso de treinamento\n",
        "    def train(self, training_inputs, training_outputs):\n",
        "        self.feed_forward(training_inputs)\n",
        "\n",
        "        # 1. Deltas de neurônios de saída\n",
        "        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
        "        for o in range(len(self.output_layer.neurons)):\n",
        "\n",
        "            # ∂E/∂zⱼ\n",
        "            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
        "\n",
        "        # 2. Deltas do neurônio oculto\n",
        "        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
        "        for h in range(len(self.hidden_layer.neurons)):\n",
        "\n",
        "            # Calcula a derivada do erro em relação à saída de cada neurônio da camada oculta\n",
        "            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
        "            d_error_wrt_hidden_neuron_output = 0\n",
        "            for o in range(len(self.output_layer.neurons)):\n",
        "                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
        "\n",
        "            # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n",
        "            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
        "\n",
        "        # 3. Atualiza os pesos dos neurônios de saída\n",
        "        for o in range(len(self.output_layer.neurons)):\n",
        "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
        "\n",
        "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
        "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
        "\n",
        "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
        "                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
        "\n",
        "        # 4. Atualiza os pesos dos neurônios ocultos\n",
        "        for h in range(len(self.hidden_layer.neurons)):\n",
        "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
        "\n",
        "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
        "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
        "\n",
        "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
        "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
        "\n",
        "    def calculate_total_error(self, training_sets):\n",
        "        total_error = 0\n",
        "        for t in range(len(training_sets)):\n",
        "            training_inputs, training_outputs = training_sets[t]\n",
        "            self.feed_forward(training_inputs)\n",
        "            for o in range(len(training_outputs)):\n",
        "                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
        "        return total_error\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyBZf2QYnrx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuronLayer:\n",
        "    def __init__(self, num_neurons, bias):\n",
        "\n",
        "        # Cada neurônio em uma camada compartilha o mesmo viés\n",
        "        self.bias = bias if bias else random.random()\n",
        "\n",
        "        self.neurons = []\n",
        "        for i in range(num_neurons):\n",
        "            self.neurons.append(Neuron(self.bias))\n",
        "\n",
        "    def inspect(self):\n",
        "        print('Neurons:', len(self.neurons))\n",
        "        for n in range(len(self.neurons)):\n",
        "            print(' Neuron', n)\n",
        "            for w in range(len(self.neurons[n].weights)):\n",
        "                print('  Weight:', self.neurons[n].weights[w])\n",
        "            print('  Bias:', self.bias)\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        outputs = []\n",
        "        for neuron in self.neurons:\n",
        "            outputs.append(neuron.calculate_output(inputs))\n",
        "        return outputs\n",
        "\n",
        "    def get_outputs(self):\n",
        "        outputs = []\n",
        "        for neuron in self.neurons:\n",
        "            outputs.append(neuron.output)\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_6ARRMAnxsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neuron:\n",
        "    def __init__(self, bias):\n",
        "        self.bias = bias\n",
        "        self.weights = []\n",
        "\n",
        "    def calculate_output(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = self.squash(self.calculate_total_net_input())\n",
        "        return self.output\n",
        "\n",
        "    def calculate_total_net_input(self):\n",
        "        total = 0\n",
        "        for i in range(len(self.inputs)):\n",
        "            total += self.inputs[i] * self.weights[i]\n",
        "        return total + self.bias\n",
        "\n",
        "    # Aplica a função logística na saída do neurônio\n",
        "    # O resultado é por vezes referido como 'net' [2] ou 'net' [1]\n",
        "    def squash(self, total_net_input):\n",
        "        return 1 / (1 + math.exp(-total_net_input))\n",
        "\n",
        "    # Determina quanto a entrada total do neurônio precisa mudar para se aproximar da saída esperada\n",
        "    #\n",
        "    # Agora que temos a derivada parcial do erro em relação à saída (∂E / ∂yⱼ) e\n",
        "    # a derivada da saída em relação à entrada líquida total (dyⱼ / dzⱼ) podemos calcular\n",
        "    # a derivada parcial do erro em relação à entrada líquida total.\n",
        "    # Esse valor também é conhecido como o delta (δ) [1]\n",
        "    # δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
        "    #\n",
        "    def calculate_pd_error_wrt_total_net_input(self, target_output):\n",
        "        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n",
        "\n",
        "    # O erro de cada neurônio é calculado pelo método do Erro Quadrático Médio:\n",
        "    def calculate_error(self, target_output):\n",
        "        return 0.5 * (target_output - self.output) ** 2\n",
        "\n",
        "    # A derivada parcial do erro em relação à saída obtida é calculada por:\n",
        "    # = 2 * 0.5 * (saída esperada - saída obtida) ^ (2 - 1) * -1\n",
        "    # = - (saída esperada - saída obtida)\n",
        "    #\n",
        "    # Alternativamente, você pode usar (target-output), mas então precisa adicioná-la durante a retropropagação [3]\n",
        "    #\n",
        "    # Observe que a saída obtida do neurônio de saída é geralmente escrita como yⱼ e a saída esperada como tⱼ então:\n",
        "    # = ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
        "    def calculate_pd_error_wrt_output(self, target_output):\n",
        "        return -(target_output - self.output)\n",
        "\n",
        "    # A função 'net' do neurônio é combinada usando a função logística para calcular a saída do neurônio:\n",
        "    # yⱼ = φ = 1 / (1 + e ^ (- zⱼ))\n",
        "    # Observe que onde ⱼ representa a saída dos neurônios em qualquer camada que estamos olhando e ᵢ representa a camada abaixo dela\n",
        "    #\n",
        "    # A derivada (não derivada parcial, uma vez que existe apenas uma variável) da saída é:\n",
        "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
        "    def calculate_pd_total_net_input_wrt_input(self):\n",
        "        return self.output * (1 - self.output)\n",
        "\n",
        "    # A função 'net' é a soma ponderada de todas as entradas para o neurônio e seus respectivos pesos:\n",
        "    # = zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n",
        "    #\n",
        "    # A derivada parcial da função 'net' com o respectivo peso (com todo o resto mantido constante) é então:\n",
        "    # = ∂zⱼ / ∂wᵢ = c + 1 * xᵢw₁ ^ (1-0) + c ... = xᵢ\n",
        "    def calculate_pd_total_net_input_wrt_weight(self, index):\n",
        "        return self.inputs[index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaDawW_s7mik",
        "colab_type": "text"
      },
      "source": [
        "##Aplicando o programa ao exemplo a seguir##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmWav9yb7VXt",
        "colab_type": "text"
      },
      "source": [
        ">![picture](https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwHtXLMhn9jq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "8cf23345-df11-4a19-b66c-0a6e408d0e24"
      },
      "source": [
        "#criando a rede neural conforme o exemplo acima\n",
        "nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], \n",
        "                    hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], \n",
        "                    output_layer_bias=0.6)\n",
        "\n",
        "#imprime a configuração da rede\n",
        "nn.inspect()                    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------\n",
            "* Inputs: 2\n",
            "------\n",
            "Hidden Layer\n",
            "Neurons: 2\n",
            " Neuron 0\n",
            "  Weight: 0.15\n",
            "  Weight: 0.2\n",
            "  Bias: 0.35\n",
            " Neuron 1\n",
            "  Weight: 0.25\n",
            "  Weight: 0.3\n",
            "  Bias: 0.35\n",
            "------\n",
            "* Output Layer\n",
            "Neurons: 2\n",
            " Neuron 0\n",
            "  Weight: 0.4\n",
            "  Weight: 0.45\n",
            "  Bias: 0.6\n",
            " Neuron 1\n",
            "  Weight: 0.5\n",
            "  Weight: 0.55\n",
            "  Bias: 0.6\n",
            "------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-MYIFvTDaCK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "0f854979-b78b-41f2-f13b-f72fcecd1439"
      },
      "source": [
        "#quantidade de épocas para o treinamento da rede\n",
        "epocs = 1001\n",
        "\n",
        "total_error = []  #cria um array para os erros totais\n",
        "\n",
        "\n",
        "#treinamento\n",
        "for i in range(epocs):\n",
        "    \n",
        "    nn.train([0.05, 0.1], [0.01, 0.99])\n",
        "    \n",
        "    total_error.append(round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\n",
        "    \n",
        "    #imprime os valores dos pesos após o treinamento\n",
        "    if i == (epocs-1):\n",
        "        print('Epochs:',i,'\\nTotal Error:', total_error[i])\n",
        "        nn.inspect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 1000 \n",
            "Total Error: 0.001112917\n",
            "------\n",
            "* Inputs: 2\n",
            "------\n",
            "Hidden Layer\n",
            "Neurons: 2\n",
            " Neuron 0\n",
            "  Weight: 0.2820847122641089\n",
            "  Weight: 0.4641694245282168\n",
            "  Bias: 0.35\n",
            " Neuron 1\n",
            "  Weight: 0.3806319502075894\n",
            "  Weight: 0.5612639004151764\n",
            "  Bias: 0.35\n",
            "------\n",
            "* Output Layer\n",
            "Neurons: 2\n",
            " Neuron 0\n",
            "  Weight: -3.064528610358445\n",
            "  Weight: -3.035163971161515\n",
            "  Bias: 0.6\n",
            " Neuron 1\n",
            "  Weight: 2.0521069380090884\n",
            "  Weight: 2.1112898194388485\n",
            "  Bias: 0.6\n",
            "------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCpMPepUBUCJ",
        "colab_type": "text"
      },
      "source": [
        "##Resultados##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1izlxnqe8w4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b4211a36-4672-4a8b-ff2a-6feb5858a439"
      },
      "source": [
        "print(total_error) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.291027774, 0.283547133, 0.275943289, 0.268232761, 0.260434393, 0.252569176, 0.244659999, 0.236731316, 0.228808741, 0.220918592, 0.213087389, 0.205341328, 0.197705769, 0.190204742, 0.182860503, 0.175693166, 0.168720403, 0.16195725, 0.155415989, 0.149106135, 0.14303449, 0.137205276, 0.131620316, 0.126279262, 0.121179847, 0.116318143, 0.111688831, 0.107285459, 0.103100677, 0.099126464, 0.095354325, 0.091775464, 0.088380932, 0.085161757, 0.082109043, 0.079214055, 0.076468284, 0.073863495, 0.071391768, 0.069045516, 0.066817506, 0.064700863, 0.062689072, 0.060775976, 0.058955766, 0.057222975, 0.05557246, 0.053999393, 0.052499243, 0.051067764, 0.049700977, 0.048395156, 0.047146814, 0.045952686, 0.044809717, 0.043715048, 0.042666004, 0.041660079, 0.040694928, 0.039768356, 0.038878306, 0.03802285, 0.037200182, 0.036408606, 0.035646534, 0.034912472, 0.03420502, 0.033522862, 0.032864759, 0.032229549, 0.031616135, 0.031023487, 0.030450633, 0.029896659, 0.0293607, 0.028841943, 0.028339618, 0.027852999, 0.0273814, 0.026924172, 0.026480701, 0.026050405, 0.025632734, 0.025227167, 0.024833208, 0.024450388, 0.024078263, 0.023716408, 0.023364423, 0.023021926, 0.022688552, 0.022363958, 0.022047812, 0.021739804, 0.021439633, 0.021147015, 0.020861679, 0.020583365, 0.020311827, 0.020046828, 0.019788143, 0.019535556, 0.01928886, 0.019047859, 0.018812364, 0.018582193, 0.018357174, 0.018137141, 0.017921935, 0.017711403, 0.017505398, 0.017303781, 0.017106417, 0.016913176, 0.016723934, 0.016538571, 0.016356973, 0.016179028, 0.016004631, 0.01583368, 0.015666075, 0.015501722, 0.015340529, 0.01518241, 0.015027277, 0.014875051, 0.014725651, 0.014579003, 0.014435032, 0.014293667, 0.01415484, 0.014018485, 0.013884538, 0.013752937, 0.013623622, 0.013496537, 0.013371624, 0.01324883, 0.013128103, 0.013009393, 0.012892649, 0.012777825, 0.012664875, 0.012553755, 0.012444421, 0.012336831, 0.012230946, 0.012126725, 0.012024131, 0.011923126, 0.011823676, 0.011725744, 0.011629298, 0.011534304, 0.011440731, 0.011348547, 0.011257723, 0.01116823, 0.011080038, 0.010993121, 0.010907451, 0.010823003, 0.010739751, 0.010657671, 0.010576737, 0.010496928, 0.01041822, 0.010340591, 0.01026402, 0.010188486, 0.010113967, 0.010040445, 0.009967899, 0.009896311, 0.009825663, 0.009755936, 0.009687113, 0.009619176, 0.00955211, 0.009485898, 0.009420524, 0.009355972, 0.009292228, 0.009229277, 0.009167104, 0.009105696, 0.009045038, 0.008985117, 0.008925921, 0.008867436, 0.00880965, 0.00875255, 0.008696126, 0.008640365, 0.008585256, 0.008530787, 0.008476948, 0.008423729, 0.008371119, 0.008319107, 0.008267684, 0.00821684, 0.008166566, 0.008116852, 0.008067689, 0.008019068, 0.007970981, 0.007923419, 0.007876373, 0.007829836, 0.007783799, 0.007738255, 0.007693196, 0.007648614, 0.007604502, 0.007560853, 0.00751766, 0.007474915, 0.007432612, 0.007390745, 0.007349306, 0.007308289, 0.007267689, 0.007227499, 0.007187712, 0.007148324, 0.007109327, 0.007070717, 0.007032488, 0.006994634, 0.006957151, 0.006920032, 0.006883273, 0.006846868, 0.006810813, 0.006775102, 0.006739731, 0.006704695, 0.00666999, 0.006635611, 0.006601553, 0.006567812, 0.006534384, 0.006501265, 0.00646845, 0.006435935, 0.006403717, 0.006371791, 0.006340154, 0.006308801, 0.006277729, 0.006246935, 0.006216414, 0.006186164, 0.00615618, 0.006126459, 0.006096999, 0.006067795, 0.006038844, 0.006010143, 0.005981689, 0.00595348, 0.005925511, 0.005897779, 0.005870283, 0.005843019, 0.005815983, 0.005789175, 0.005762589, 0.005736225, 0.005710078, 0.005684148, 0.00565843, 0.005632922, 0.005607623, 0.005582529, 0.005557638, 0.005532947, 0.005508455, 0.005484159, 0.005460056, 0.005436145, 0.005412423, 0.005388888, 0.005365538, 0.00534237, 0.005319384, 0.005296575, 0.005273944, 0.005251487, 0.005229203, 0.005207089, 0.005185144, 0.005163366, 0.005141753, 0.005120304, 0.005099016, 0.005077887, 0.005056917, 0.005036103, 0.005015443, 0.004994936, 0.00497458, 0.004954374, 0.004934316, 0.004914404, 0.004894638, 0.004875014, 0.004855532, 0.004836191, 0.004816988, 0.004797923, 0.004778994, 0.004760199, 0.004741538, 0.004723008, 0.004704609, 0.004686339, 0.004668196, 0.00465018, 0.00463229, 0.004614523, 0.004596879, 0.004579356, 0.004561953, 0.00454467, 0.004527504, 0.004510455, 0.004493522, 0.004476703, 0.004459997, 0.004443403, 0.004426921, 0.004410548, 0.004394284, 0.004378129, 0.004362079, 0.004346136, 0.004330298, 0.004314563, 0.004298931, 0.0042834, 0.004267971, 0.004252642, 0.004237411, 0.004222278, 0.004207243, 0.004192304, 0.00417746, 0.00416271, 0.004148054, 0.00413349, 0.004119019, 0.004104638, 0.004090348, 0.004076146, 0.004062034, 0.004048009, 0.004034071, 0.004020219, 0.004006452, 0.00399277, 0.003979172, 0.003965657, 0.003952224, 0.003938873, 0.003925603, 0.003912413, 0.003899303, 0.003886271, 0.003873317, 0.003860441, 0.003847642, 0.003834918, 0.00382227, 0.003809697, 0.003797198, 0.003784772, 0.003772419, 0.003760139, 0.00374793, 0.003735792, 0.003723725, 0.003711727, 0.003699799, 0.003687939, 0.003676147, 0.003664423, 0.003652766, 0.003641175, 0.00362965, 0.00361819, 0.003606796, 0.003595465, 0.003584198, 0.003572994, 0.003561853, 0.003550774, 0.003539757, 0.003528801, 0.003517905, 0.00350707, 0.003496294, 0.003485578, 0.00347492, 0.003464321, 0.003453779, 0.003443295, 0.003432868, 0.003422497, 0.003412182, 0.003401923, 0.003391719, 0.003381569, 0.003371474, 0.003361433, 0.003351445, 0.00334151, 0.003331628, 0.003321797, 0.003312019, 0.003302292, 0.003292616, 0.003282991, 0.003273416, 0.003263891, 0.003254416, 0.003244989, 0.003235611, 0.003226282, 0.003217, 0.003207767, 0.00319858, 0.003189441, 0.003180348, 0.003171301, 0.003162301, 0.003153345, 0.003144435, 0.00313557, 0.00312675, 0.003117974, 0.003109241, 0.003100553, 0.003091907, 0.003083305, 0.003074745, 0.003066228, 0.003057753, 0.003049319, 0.003040927, 0.003032576, 0.003024266, 0.003015996, 0.003007767, 0.002999578, 0.002991429, 0.002983319, 0.002975248, 0.002967216, 0.002959222, 0.002951267, 0.00294335, 0.002935471, 0.002927629, 0.002919825, 0.002912058, 0.002904327, 0.002896633, 0.002888976, 0.002881354, 0.002873768, 0.002866218, 0.002858703, 0.002851223, 0.002843778, 0.002836367, 0.002828991, 0.002821649, 0.002814341, 0.002807066, 0.002799825, 0.002792617, 0.002785442, 0.0027783, 0.00277119, 0.002764113, 0.002757068, 0.002750055, 0.002743073, 0.002736123, 0.002729205, 0.002722317, 0.00271546, 0.002708634, 0.002701839, 0.002695073, 0.002688338, 0.002681633, 0.002674957, 0.002668311, 0.002661694, 0.002655107, 0.002648548, 0.002642018, 0.002635517, 0.002629044, 0.002622599, 0.002616182, 0.002609794, 0.002603432, 0.002597099, 0.002590793, 0.002584513, 0.002578261, 0.002572036, 0.002565837, 0.002559665, 0.00255352, 0.0025474, 0.002541306, 0.002535239, 0.002529196, 0.00252318, 0.002517189, 0.002511223, 0.002505282, 0.002499366, 0.002493475, 0.002487608, 0.002481766, 0.002475948, 0.002470155, 0.002464385, 0.002458639, 0.002452917, 0.002447219, 0.002441544, 0.002435892, 0.002430264, 0.002424658, 0.002419076, 0.002413516, 0.002407979, 0.002402464, 0.002396972, 0.002391502, 0.002386054, 0.002380628, 0.002375223, 0.002369841, 0.00236448, 0.002359141, 0.002353822, 0.002348526, 0.00234325, 0.002337995, 0.002332761, 0.002327548, 0.002322355, 0.002317183, 0.002312031, 0.002306899, 0.002301788, 0.002296697, 0.002291625, 0.002286574, 0.002281542, 0.002276529, 0.002271536, 0.002266563, 0.002261608, 0.002256673, 0.002251757, 0.00224686, 0.002241981, 0.002237122, 0.00223228, 0.002227458, 0.002222654, 0.002217868, 0.0022131, 0.00220835, 0.002203619, 0.002198905, 0.002194209, 0.002189531, 0.002184871, 0.002180227, 0.002175602, 0.002170994, 0.002166402, 0.002161828, 0.002157272, 0.002152732, 0.002148209, 0.002143702, 0.002139213, 0.002134739, 0.002130283, 0.002125843, 0.002121419, 0.002117011, 0.00211262, 0.002108244, 0.002103885, 0.002099542, 0.002095214, 0.002090902, 0.002086606, 0.002082325, 0.00207806, 0.00207381, 0.002069575, 0.002065356, 0.002061152, 0.002056963, 0.002052788, 0.002048629, 0.002044485, 0.002040355, 0.002036241, 0.00203214, 0.002028055, 0.002023983, 0.002019927, 0.002015884, 0.002011856, 0.002007842, 0.002003842, 0.001999856, 0.001995884, 0.001991926, 0.001987981, 0.001984051, 0.001980134, 0.001976231, 0.001972341, 0.001968465, 0.001964602, 0.001960752, 0.001956916, 0.001953093, 0.001949283, 0.001945486, 0.001941703, 0.001937932, 0.001934174, 0.001930429, 0.001926696, 0.001922976, 0.001919269, 0.001915575, 0.001911893, 0.001908223, 0.001904566, 0.001900921, 0.001897288, 0.001893668, 0.00189006, 0.001886463, 0.001882879, 0.001879307, 0.001875747, 0.001872198, 0.001868662, 0.001865137, 0.001861623, 0.001858122, 0.001854632, 0.001851153, 0.001847686, 0.00184423, 0.001840786, 0.001837353, 0.001833931, 0.00183052, 0.001827121, 0.001823732, 0.001820355, 0.001816989, 0.001813633, 0.001810289, 0.001806955, 0.001803632, 0.001800319, 0.001797018, 0.001793727, 0.001790446, 0.001787176, 0.001783917, 0.001780668, 0.001777429, 0.001774201, 0.001770983, 0.001767775, 0.001764578, 0.00176139, 0.001758213, 0.001755046, 0.001751888, 0.001748741, 0.001745603, 0.001742476, 0.001739358, 0.00173625, 0.001733152, 0.001730063, 0.001726984, 0.001723915, 0.001720855, 0.001717805, 0.001714764, 0.001711733, 0.001708711, 0.001705698, 0.001702695, 0.001699701, 0.001696716, 0.00169374, 0.001690774, 0.001687816, 0.001684868, 0.001681929, 0.001678998, 0.001676077, 0.001673164, 0.00167026, 0.001667366, 0.001664479, 0.001661602, 0.001658733, 0.001655873, 0.001653022, 0.001650179, 0.001647345, 0.001644519, 0.001641702, 0.001638893, 0.001636093, 0.001633301, 0.001630517, 0.001627742, 0.001624974, 0.001622216, 0.001619465, 0.001616722, 0.001613988, 0.001611261, 0.001608543, 0.001605833, 0.00160313, 0.001600436, 0.001597749, 0.001595071, 0.0015924, 0.001589737, 0.001587082, 0.001584435, 0.001581795, 0.001579163, 0.001576538, 0.001573922, 0.001571312, 0.001568711, 0.001566117, 0.00156353, 0.001560951, 0.001558379, 0.001555815, 0.001553258, 0.001550708, 0.001548166, 0.001545631, 0.001543103, 0.001540582, 0.001538069, 0.001535563, 0.001533064, 0.001530572, 0.001528087, 0.001525609, 0.001523138, 0.001520674, 0.001518217, 0.001515767, 0.001513324, 0.001510887, 0.001508458, 0.001506035, 0.001503619, 0.00150121, 0.001498808, 0.001496412, 0.001494023, 0.001491641, 0.001489265, 0.001486896, 0.001484533, 0.001482177, 0.001479827, 0.001477484, 0.001475148, 0.001472818, 0.001470494, 0.001468177, 0.001465866, 0.001463561, 0.001461263, 0.001458971, 0.001456685, 0.001454405, 0.001452132, 0.001449865, 0.001447604, 0.001445349, 0.0014431, 0.001440857, 0.001438621, 0.00143639, 0.001434166, 0.001431947, 0.001429735, 0.001427528, 0.001425327, 0.001423133, 0.001420944, 0.001418761, 0.001416583, 0.001414412, 0.001412246, 0.001410086, 0.001407932, 0.001405784, 0.001403641, 0.001401504, 0.001399373, 0.001397247, 0.001395127, 0.001393013, 0.001390904, 0.0013888, 0.001386702, 0.00138461, 0.001382523, 0.001380442, 0.001378366, 0.001376295, 0.00137423, 0.001372171, 0.001370116, 0.001368067, 0.001366023, 0.001363985, 0.001361952, 0.001359924, 0.001357901, 0.001355884, 0.001353872, 0.001351865, 0.001349863, 0.001347866, 0.001345875, 0.001343888, 0.001341907, 0.001339931, 0.001337959, 0.001335993, 0.001334032, 0.001332076, 0.001330125, 0.001328178, 0.001326237, 0.001324301, 0.001322369, 0.001320442, 0.001318521, 0.001316604, 0.001314692, 0.001312785, 0.001310882, 0.001308984, 0.001307092, 0.001305203, 0.00130332, 0.001301441, 0.001299567, 0.001297698, 0.001295833, 0.001293973, 0.001292118, 0.001290267, 0.001288421, 0.001286579, 0.001284742, 0.00128291, 0.001281082, 0.001279258, 0.001277439, 0.001275625, 0.001273815, 0.001272009, 0.001270208, 0.001268411, 0.001266619, 0.001264831, 0.001263048, 0.001261268, 0.001259493, 0.001257723, 0.001255957, 0.001254195, 0.001252437, 0.001250684, 0.001248935, 0.00124719, 0.001245449, 0.001243712, 0.00124198, 0.001240252, 0.001238528, 0.001236808, 0.001235093, 0.001233381, 0.001231674, 0.00122997, 0.001228271, 0.001226576, 0.001224884, 0.001223197, 0.001221514, 0.001219835, 0.00121816, 0.001216489, 0.001214822, 0.001213158, 0.001211499, 0.001209844, 0.001208192, 0.001206544, 0.001204901, 0.001203261, 0.001201625, 0.001199993, 0.001198365, 0.00119674, 0.001195119, 0.001193503, 0.001191889, 0.00119028, 0.001188675, 0.001187073, 0.001185475, 0.00118388, 0.001182289, 0.001180702, 0.001179119, 0.00117754, 0.001175963, 0.001174391, 0.001172822, 0.001171257, 0.001169696, 0.001168138, 0.001166584, 0.001165033, 0.001163486, 0.001161942, 0.001160402, 0.001158865, 0.001157332, 0.001155803, 0.001154276, 0.001152754, 0.001151235, 0.001149719, 0.001148207, 0.001146698, 0.001145192, 0.00114369, 0.001142192, 0.001140697, 0.001139205, 0.001137716, 0.001136231, 0.001134749, 0.001133271, 0.001131796, 0.001130324, 0.001128855, 0.00112739, 0.001125928, 0.00112447, 0.001123014, 0.001121562, 0.001120113, 0.001118667, 0.001117225, 0.001115786, 0.001114349, 0.001112917]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJvz9fi3-C3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "36a56c42-873f-406a-b4d4-d65f82fd4103"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plt.style.use('fivethirtyeight')\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "\n",
        "plt.plot(total_error)\n",
        "plt.ylabel('Erro total')\n",
        "plt.xlabel('número de épocas')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEDCAYAAAA4FgP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nARGRwv1/L+wyS7oUI/MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObah1yC7r09",
        "colab_type": "text"
      },
      "source": [
        "###Referências:###\n",
        "\n",
        "    link: <https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/>\n",
        "    link: <https://github.com/mattm/simple-neural-network>\n",
        "    "
      ]
    }
  ]
}